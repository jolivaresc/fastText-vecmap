{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "__author__ = \"Olivares Castillo José Luis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(file,vocabulary=None,dtype=np.float64):\n",
    "    words = []\n",
    "    header = file.readline().strip().decode().split()\n",
    "    count = int(header[0])# if vocabulary is None else min(len(set(vocabulary)),int(header[0]))\n",
    "    dim = int(header[1])\n",
    "    matrix = np.empty((count,dim), dtype=dtype) if vocabulary is None else []\n",
    "    for i in range(count):\n",
    "        word, vec = file.readline().decode().split(\" \", 1)\n",
    "        #word, vec = file.readline().strip.decode(encoding='UTF-8',errors='strict').split(\" \", 1)\n",
    "        if vocabulary is None:\n",
    "            words.append(word)\n",
    "            matrix[i] = np.fromstring(vec, sep=\" \", dtype=dtype)\n",
    "        elif word in vocabulary:\n",
    "            words.append(word)\n",
    "            matrix.append(np.fromstring(vec,sep=\" \", dtype=dtype))\n",
    "    file.close()\n",
    "    return (words,matrix) if vocabulary is None else (words,np.array(matrix,dtype=dtype))\n",
    "\n",
    "\n",
    "def open_file(source):\n",
    "    if source.__eq__(\"en-wiki\"):\n",
    "        file = ZipFile(\"datasets/wiki-news-300d-1M-subword.vec.zip\")\\\n",
    "                        .open(\"wiki-news-300d-1M-subword.vec\")\n",
    "    if source.__eq__(\"en-crawl\"):\n",
    "        file = ZipFile(\"datasets/crawl-300d-2M.vec.zip\")\\\n",
    "                        .open(\"crawl-300d-2M.vec\")\n",
    "    elif source.__eq__(\"it\"):\n",
    "        file = gzip.open(\"datasets/en-it/cc.it.300.vec.gz\")\n",
    "    elif source.__eq__(\"de\"):\n",
    "        file = gzip.open(\"datasets/en-de/cc.de.300.vec.gz\")\n",
    "    elif source.__eq__(\"fi\"):\n",
    "        file = gzip.open(\"datasets/en-fi/cc.fi.300.vec.gz\")\n",
    "    elif source.__eq__(\"es\"):\n",
    "        file = gzip.open(\"datasets/en-es/cc.es.300.vec.gz\")\n",
    "    else:\n",
    "        raise ValueError(\"{} no encontrado\".format(source))\n",
    "    return io.BufferedReader(file)\n",
    "\n",
    "        \n",
    "def get_lexicon(source):\n",
    "    path = \"datasets/dictionaries/\"\n",
    "    if source.__eq__(\"en-it.train\"):\n",
    "        src,trg = load_lexicon(path+\"en-it.train.drive.txt\")\n",
    "    elif source.__eq__(\"en-it.test\"):\n",
    "        src,trg = load_lexicon(path+\"en-it.test.drive.txt\")\n",
    "    elif source.__eq__(\"en-de.test\"):\n",
    "        src,trg = load_lexicon(path+\"en-de.test.txt\")\n",
    "    elif source.__eq__(\"en-de.train\"):\n",
    "        src,trg = load_lexicon(path+\"en-de.train.txt\")\n",
    "    elif source.__eq__(\"en-es.test\"):\n",
    "        src,trg = load_lexicon(path+\"en-es.test.txt\")\n",
    "    elif source.__eq__(\"en-es.train\"):\n",
    "        src,trg = load_lexicon(path+\"en-es.train.txt\")\n",
    "    elif source.__eq__(\"en-fi.test\"):\n",
    "        src,trg = load_lexicon(path+\"en-fi.test.txt\")\n",
    "    elif source.__eq__(\"en-fi.train\"):\n",
    "        src,trg = load_lexicon(path+\"en-fi.train.txt\")\n",
    "    else:\n",
    "        raise ValueError(\"{} no encontrado\".format(source))\n",
    "    return (src,trg)\n",
    "    \n",
    "\n",
    "def load_lexicon(source):\n",
    "    src, trg = [], []\n",
    "    with open(source, \"r\", encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            src.append(line.split()[0])\n",
    "            trg.append(line.split()[1])\n",
    "    return (src, trg)\n",
    "\n",
    "\n",
    "def get_vectors(lexicon, words, embeddings, dtype='float'):\n",
    "    matrix = np.empty((len(lexicon), embeddings.shape[1]), dtype=dtype)\n",
    "    for i in range(len(lexicon)):\n",
    "        if lexicon[i] in words:\n",
    "            matrix[i] = embeddings[words.index(lexicon[i])]\n",
    "    return np.asarray(matrix, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_lex,trg_lex = get_lexicon(\"en-de.train\")\n",
    "len((src_lex)),len((trg_lex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex = trg_lex\n",
    "file = open_file(\"de\")\n",
    "words, vec = read(file)\n",
    "matrix = get_vectors(list((lex)),words,vec)\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    }
   ],
   "source": [
    "r=0\n",
    "for _,i in enumerate(trg_lex):\n",
    "    if i not in words:\n",
    "        #print(_,i)\n",
    "        r+=1\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/en-de/de.embeddings.300d.5k.train\",\"w\") as f:\n",
    "    for _ in range(matrix.shape[0]):\n",
    "        if _.__ne__(matrix.shape[0] - 1):\n",
    "            f.write(lex[_]+\" \"+\" \".join(map(str,matrix[_]))+\"\\n\")\n",
    "        else:\n",
    "            f.write(lex[_]+\" \"+\" \".join(map(str,matrix[_])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000000, 300), 2000000, (1869, 300))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vec.shape,len(src_words),src_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm=list(set(src_lex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[tm,src_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[set(a) for a in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudeten\n",
      "ravalomanana\n",
      "brigitte\n",
      "jti\n",
      "brueghel\n",
      "antioquia\n",
      "carcassonne\n",
      "raynal\n",
      "inés\n",
      "koizumi\n",
      "garang\n",
      "sète\n",
      "fukuyama\n",
      "racs\n",
      "véronique\n",
      "saakashvili\n",
      "elmar\n",
      "unclos\n",
      "gaborone\n",
      "akim\n",
      "ekaterinburg\n",
      "romanies\n",
      "lisi\n",
      "tamino\n",
      "matera\n",
      "enp\n",
      "guterres\n",
      "canarian\n",
      "kabylie\n",
      "malpensa\n",
      "mif\n",
      "prizren\n",
      "jpa\n",
      "thüringen\n",
      "scowcroft\n",
      "alois\n",
      "mazen\n",
      "ceausescu\n",
      "vukovar\n",
      "tindouf\n",
      "chios\n",
      "nanterre\n",
      "chandrika\n",
      "beckenbauer\n",
      "mossadeq\n",
      "poso\n",
      "kfor\n",
      "fassa\n",
      "scheveningen\n",
      "andreotti\n",
      "katerina\n",
      "bassam\n",
      "berliners\n",
      "valletta\n",
      "schreyer\n",
      "blaise\n",
      "lumumba\n",
      "kostunica\n",
      "gaullists\n",
      "tenzin\n",
      "vercelli\n",
      "antonello\n",
      "bogotá\n",
      "nlg\n",
      "rosati\n",
      "ahmet\n",
      "kumaratunga\n",
      "giulia\n",
      "ratko\n",
      "tegucigalpa\n",
      "apulia\n",
      "roxana\n",
      "lindh\n",
      "olof\n",
      "beazley\n",
      "isfahan\n",
      "csps\n",
      "valenciennes\n",
      "jaruzelski\n",
      "liszt\n",
      "lahoud\n",
      "tornio\n",
      "interreg\n",
      "fonzie\n",
      "jacek\n",
      "marburg\n",
      "lennart\n",
      "aspe\n",
      "gbagbo\n",
      "hélène\n",
      "adige\n",
      "cpmp\n",
      "melchior\n",
      "jörg\n",
      "perpignan\n",
      "osip\n",
      "valery\n",
      "ingushetia\n",
      "hristo\n",
      "yalta\n",
      "idriss\n",
      "megawati\n",
      "mendieta\n",
      "radovan\n",
      "izetbegovic\n",
      "magda\n",
      "alber\n",
      "thabo\n",
      "euskadi\n",
      "heysel\n",
      "muscovites\n",
      "ngawang\n",
      "bessarabia\n",
      "alde\n",
      "billingham\n",
      "lyudmila\n",
      "agis\n",
      "hager\n",
      "vélez\n",
      "rehn\n",
      "walesa\n",
      "allawi\n",
      "angulo\n",
      "rennes\n",
      "renzo\n",
      "tamayo\n",
      "cpn\n",
      "chisinau\n",
      "nazarbayev\n",
      "unrwa\n",
      "leonidas\n",
      "tarragona\n",
      "daghestan\n",
      "bouteflika\n",
      "kabila\n",
      "ecevit\n",
      "copd\n",
      "sorbonne\n",
      "karamanlis\n",
      "agusta\n",
      "saks\n",
      "rovaniemi\n",
      "méndez\n",
      "miroslav\n",
      "omc\n",
      "bercy\n",
      "montalvo\n",
      "einaudi\n",
      "aom\n",
      "kouchner\n",
      "farouk\n",
      "clarín\n",
      "igad\n",
      "viareggio\n",
      "ribeira\n",
      "rosales\n",
      "nasrallah\n",
      "rafah\n",
      "ruy\n",
      "eeurope\n",
      "auc\n",
      "ncbs\n",
      "docomo\n",
      "jrc\n",
      "unwin\n",
      "charybdis\n",
      "ogoni\n",
      "togolese\n",
      "michelis\n",
      "wilfredo\n",
      "leyla\n",
      "brok\n",
      "yilmaz\n",
      "michèle\n",
      "atocha\n",
      "genk\n",
      "núñez\n",
      "katyn\n",
      "dassault\n",
      "iiib\n",
      "unodc\n",
      "jaurès\n",
      "dutchmen\n",
      "bavarians\n",
      "benes\n",
      "jiabao\n",
      "vitoria\n",
      "papadopoulos\n",
      "pagano\n",
      "cotonou\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "w=0\n",
    "for i in tm:\n",
    "    if i not in src_words:\n",
    "        print(i)\n",
    "        w+=1\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1310+190"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
